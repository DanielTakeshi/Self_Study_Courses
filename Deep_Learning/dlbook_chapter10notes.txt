****************************************************************
* NOTES ON CHAPTER 10: Recurrent and Recursive Neural Networks *
****************************************************************

I need to understand the parameter sharing and how RNNs (and their variants) can
be "combined" into other areas. The parameter sharing is key, as it allows for
*generalization*. CNNs share parameters with the weight filters across the
grids; RNNs share parameters across timesteps.

Quick note: I think they're using minibatch sizes of 1 to simplify all notation
and exposition here. That's fine with me. Think of x as:

[ x^1 x^2 ... x^T ]

where superscripts indicate time. Note that each x^i itself could be a vector!

Section 10.2, Recurrent Neural Networks

It's important to understand the *computational graphs* involved with RNNs. I
understand them as directed acyclic graphs, so how does this extend with
recurrence? It's easier to think of them when we unroll (i.e. "unfold") the
computational graphs. See Figure 10.2 as an example (I was able to get this
without looking at the figure). They also use a more succinct "recurrent graph"
representation.

RNN Design Patterns, also kind of described in Andrej Karpathy's blog post:

- Producing an output at each time step, and having recurrent connections
  between hidden layers. This is Figure 10.3, which I correctly predicted in
  advance minus the loss and y stuff. They have losses for *each* time step.
  Note the three matrix multiplies that are there, with the *same* respective
  matrices repeated across time. Also, we're using the softmax, so assume the
  output is discrete at each time step, e.g. o(t) could be the categorical
  distribution over the 26 letters in the alphabet.

- Same as above, except recurrent connections are from outputs to hidden layers,
  so we still have three matrices but the "arrows" in the computational graph
  change. This is *less powerful**. Why?? Think: the former allows hidden to
  hidden, so the hidden stuff can be very rich. The output only lets hidden to
  output to hidden, so the output is the input and may be less rich. That seems
  intuitive.

- Same as the first one (hidden to hidden connections) except we now have one
  output. That's useful to summarize, such as if we're doing sequence
  classification.

Now develop the equations, e.g. f(b + Wh + Ux) where h is from the *previous*
time step and x is the *current* time step, and f is the *activation* function.
Yes, it's all familiar to me. They mention, though, that backpropagation is very
expensive. They call the naive way (applying it on the unrolled computational
graph) as "backpropagation through time."

How to compute the gradient? They give us an example, thank goodness. Comments:

- Note that L = L(1) + L(2) + ... + L(\tau) so yes, dL/dL(t) = 1 for all t. Each
  L(t) is a negative log probability for that output at that time.

- The next equation (10.18) also makes sense, here i is the component in the
  vector, so we're in the univariate case.

- Equation 10.19 is good, keep in mind that here we have to be careful with the
  timestep. For other h(t), we need to add two gradients due to two incoming
  terms (because of two *outgoing* terms in the *forward* pass). Thus, the
  matrices V and W will be present in some form.

- The next part about using dummy variables for t is slightly confusing but it
  should just mean that the total contribution for these parameters are based on
  their sum across each time. Yeah, looking at the book again it's just a
  notation issue to help us out. For all those gradients, we have a final sum
  over t, where each term in the sum is a matrix/vector of the same size as the
  variable we're taking the gradient w.r.t.

PS: when reading this, don't be confused by the notation. Look at the "notation"
chapter online.

RNNs as directed graphical models? This section is about expressing them as
well-defined directed graphical models, and there are a few subtleties. This is
WITHOUT any inputs, BTW ... probably just for intuition?

They go through an example predicting a sequence of scalars. With the naive
unrolled (directed) graphical model, we're applying the chain rule of
probability and so it's very inefficient. RNNs provide better (in many metrics,
but particularly efficiency) ways to express such distributions with directed
graphical models by introducing deterministic connections (remember, the hidden
states are deterministic).

With RNNs, parameter sharing is a huge advantage, but the downside is that
optimizing is hard because we make a potentially strong assumption that at each
time step, the distribution embedded in the RNN remains stationary.

The last bit here to get it into a well-defined graphical model is to figure out
the length of the RNN. The paper presents three options, all of which seem
obvious (though I'm ignoring lots of details, etc.).

The next subsection (10.2.4) after this is about the more realistic setting of
having x (input), so we're also modeling p(y|x). I think it's trying to stick
with the graphical model setting. Also, note that the second option in the list
of three things is what we did in CS 231n, Assignment 3, with the image
captioning portion. Actually, the first option would seem better, which
translates the input image to a vector as input to *all* hidden states, but
that's harder to implement.

I was quite confused about Figure 10.9, as to why are we considering the y(t)s
as inputs?? However, it seems like it's because we want to model p(y|x) and,
well, y is the ground truth. I'm just having trouble translating this to code,
or maybe that's not what I should be doing, and instead just think of it as a
graphical model? To think of it as code, I'd need the other case we had earlier
where the *output* or *hidden state* was the input to the hidden state, not the
actual target (which is to be compared with the output).

Section 10.3: Bidirectional RNNs

Bidirectional RNNs help us model the output y(t) when that output may also
*depend on future times* t+1, t+2, etc., such as with speech recognition where
we need to peek ahead a bit. Don't use a fixed window, though, they say:

> This allows the output units o(t) to compute a representation that depends on
> both the past and the future but is most sensitive to the input values around
> time t, without having to specify a fixed-size window around t.

Nice!

Section 10.4: Encoder-Decoder Sequence-to-Sequence Architectures

Use these to avoid the restriction of fixed sequence sizes for the inputs x (or
x(t)). This is their main benefit/innovation, the lengths n_x and n_y (see
Figure 10.12 if confused on this notation) **can vary**; if the training
data consists of a bunch of sequences that are of similar or different lengths,
the RNN will learn to mirror that training data.  Side note: the first relevant
paper on this (from 2014) called it "Encoder-Decoder" while the second one
called it "Sequence-to-Sequence". I skimmed that second one, from Sutskever et
al, NIPS 2014 last year, though maybe I should re-read it. Both papers are
highly-cited.

Connection with Section 10.2.4: we have a fixed-sized context vector C (well,
usually) coming out of the encoder. Well, C is input to the decoder, and this is
*precisely* the vector-to-sequence RNN architecture we talked about in that
sub-section!

How can the encoder deal with varying sizes n_x? If you think about it, it's
just applying the RNN update over and over again to produce a fixed hidden state
of the same size. At time t, we have processed x(1),...,x(t), and have hidden
state h(t). (We're ignoring the earlier hidden states for simplicity.) Then the
next time t+1, let's say that's our last one. Then we pass in h(t+1). So there's
no issue with getting different sized inputs, because all that matters is (a)
that we can repeatedly apply the RNN update, which is a for loop over the input
sequence, and (b) that we take a fixed sized input to the decoder, which we can
do with our final hidden state!

Section 10.5: Deep Recurrent Neural Networks

In all likelihood, I will not be dealing with these, but it might be worth
knowing how deep we can go with RNNs, just like how I learned about the very
deep GoogLeNet and the **ultra** deep ResNet. When we talk about depth, we mean
adding more layers (w.r.t. the unrolled graph perspective) to the three
components: input to hidden, hidden to hidden, and/or hidden to output. This
might make learning hard, so one option is to introduce skip connections like
in ResNets (man, I'm glad I reviewed ResNets).

Section 10.6: Recursive Neural Networks

Recursive Neural Networks, which we **do not** abbreviate as RNN, are a
generalization of RNNs with a different computational graph "flavor" that looks
like a tree rather than a chain.

Section 10.7: Challenge of Long-Term Depenedencies

Why is it hard? Here are some relevant quotes:

> The basic problem is that gradients propagated over many stages tend to either
> vanish (most of the time) or explode (rarely, but with much damage to the
> optimization). [...] the difficulty with long-term dependencies arises from
> the exponentially smaller weights given to long-term interactions (involving
> the multiplication of many Jacobians) compared to short-term ones. [...]
> Recurrent networks involve the composition of the same function multiple
> times, once per time step. These compositions can result in extremely
> nonlinear behavior, as illustrated in figure 10.15.

This section describes the problem, and the subsequent sections (I assume 10.8
through 10.12, judging from the LSTMs here) describe ways to solve it.

They present a simplified analysis with matrix eigendecomposition, where we
assume no activations. Then yes, gradients can explode if eigenvalues are
greater than one or vanish if they are less than zero. Andrej Karpathy said
something similar in his medium blog post (why does he bother with medium?).

No free lunch:

> One may hope that the problem can be avoided simply by staying in a region of
> parameter space where the gradients do not vanish or explode. Unfortunately,
> in order to store memories in a way that is robust to small perturbations, the
> RNN must enter a region of parameter space where gradients vanish (Bengio et
> al., 1993, 1994).

It's a bit annoying that we are simplifying here by ignoring the activation
functions, but I guess Bengio's old papers address activation functions?

Section 10.8: Echo State Networks

I skimmed this section. It's quite high-level and not that important to me.

Section 10.9: Leaky Units, Multiple Time Scales

I like this explanation:

> One way to deal with long-term dependencies is to design a model that operates
> at multiple time scales, so that some parts of the model operate at
> fine-grained time scales and can handle small details, while other parts
> operate at coarse time scales and transfer information from the distant past
> to the present more efficiently.

Oddly enough, they don't cite the ResNet paper?!?

They can add skip connections (i.e. adding edges to the RNN). Or they can remove
edges from the RNN, which might have similar positive effects as skip
connections.

Section 10.10: LSTMs (finally!), Gated Recurrent Unit RNNs

As of this writing (2016), these two RNNs are the most effective RNNs we have
for practical applications involving sequences.

Gated Recurrent Unit (GRU):

- Main idea:

  > [...] gated RNNs are based on the idea of creating paths through time that
  > have derivatives that neither vanish nor explode.

- The RNN needs to *learn* when to forget and discard the past (it can't
  remember everything, after all!).

- Another quote:

  > The main difference with the LSTM is that a single gating unit
  > simultaneously controls the forgetting factor and the decision to update the
  > state unit.

Long Short-Term Memory (LSTM):

- See Figure 10.16 for the block diagram. It's still very confusing despite how
  I implemented it in CS 231n. I'm amazed that these work at all.

- Like GRUs, LSTMs need to *learn* when to forget.

- It uses self-loops to enable paths to flow for long durations. By flow, I mean
  not only the forward pass, but the *backward* pass.

The authors' conclusion is to simply stick with GRUs or LSTMs.

Section 10.11: Optimization for Long-Term Dependencies

They talk about how to improve optimization, such as with second-order methods
and clipping gradients. (Be careful, taking the average of a bunch of clipped
gradients means gradients that were larger have their contributions removed; see
the discussion in the textbook.)

I wouldn't put too much stock into this, though, because the authors say:

> This is part of a continuing theme in machine learning that it is often much
> easier to design a model that is easy to optimize than it is to design a more
> powerful optimization algorithm.

In fact it seems like it's easier to train LSTMs using simple SGD rather than
use a more complicated optimization algorithm. PS: is ADAM used with RNNs?

Section 10.12: Explicit Memory

Philosophical quote:

> Neural networks excel at storing implicit knowledge. However, they struggle to
> memorize facts.

This section introduces **Memory Networks** and **Neural Turing Machines**.

For NTMs, note that:

> It is difficult to optimize functions that produce exact, integer addresses.
> To alleviate this problem, NTMs actually read to or write from many memory
> cells simultaneously. To read, they take a weighted average of many cells. To
> write, they modify multiple cells by different amounts

Yeah, it's basically **soft attention**.

Conclusion of the chapter:

> Recurrent neural networks provide a way to extend deep learning to sequential
> data. They are the last major tool in our deep learning toolbox. Our
> discussion now moves to how to choose and use these tools and how to apply
> them to real-world tasks.

Whew!
