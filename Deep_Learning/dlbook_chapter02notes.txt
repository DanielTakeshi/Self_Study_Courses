**************************************
* NOTES ON CHAPTER 2: Linear Algebra *
**************************************

This chapter was pure review for me, but some highlights and insights:

- They talk about tensors but I'm kind of familiar with them already, mostly
  when I have to deal with numpy arrays that have at least three coordinate
  dimensions (or four, in some deep learning applications with images).

- Columns of A can be thought of as different directions we're spanning out of
  the origin, and the components of x (as in the matrix-vector product Ax)
  indicate how far we move in those directions.

- We say "orthogonal" matrices, but there's no terminology for matrices whose
  columns and/or rows are mutually orthogonal, but *not* orthonormal.

- Don't forget **eigendecompositions**! They're very important. Interesting
  intuition:

  > [...] we can also decompose matrices in ways that show us information about
  > their functional properties that is not obvious from the representation of
  > the matrix as an array of elements.

  Eigendecomposition of matrix: A = V * diag(eig-vals) * V^{-1}, where V
  has columns which correspond to (right) eigenvectors of A.

  Not every matrix can be decomposed this way, but we're usually concerned with
  real symmetric A. In fact, in that case we can say even more: we can construct
  an *orthogonal* V so our V^{-1} turns into the easier-to-deal-with V^T matrix.

- An alternative, and more generally applicable decomposition, is the SVD. (Why
  is it more general? Well, every real matrix has an SVD, including non-square
  ones, but non-square matrices have undefined eigendecompositions.) In their
  formulation, the inner matrix of singular values is rectangular in general
  (other books/references have *square* matrices, but the definitions are
  essentially equivalent).

- Moore-Penrose pseudoinverse helps us (sometimes) solve linear equations for
  non-square matrices, in which case the "normal" inverse cannot be defined. Use
  the formula A^+ = V * D^+ * U^T for the pseudoinverse. When A is a fat matrix,
  the solution x = A^+ * y provides us with the minimum Euclidean norm solution
  (I must have forgotten this fact).

- For the trace, don't forget about the **cyclic property**!!!

- The chapter concludes with an example of **Principal Components Analysis**,
  i.e. how to apply lossy compression to a set of data points while losing as
  little information as possible. By "compression" we refer to shrinking points
  from R^m into R^n where n < m. This is necessarily lossy. To optimally encode
  a vector, use f(x) = D^Tx, which we determined from L2 norm minimization. The
  decoder is g(c) = Dc = DD^Tx which reconstructs an approximated version of the
  input from the compression.  Then the next (and final) step is to find D. They
  do this by also using an L2 minimization. They provide some nice tips on how
  to write out optimization problems nicely and compactly. This is again review
  for me.

Well, I'm pleased with this chapter. =) I should expand upon some of these
concepts in personal blog posts, particularly that last part (the proof by
induction).
