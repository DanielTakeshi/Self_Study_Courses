********************************************************************************
* These are notes based on:
* 
* Kenneth A. Ross
* Elementary Analysis: The Theory of Calculus
* Second Edition, 2013
********************************************************************************


*************
* CHAPTER 1 *
*************

I skimmed this chapter and I should know just about everything from it. It
includes:

- Natural numbers

- Simple induction

- Rational numbers (also the definition of an "algebraic number")

- The "Rational Zeros" theorem, which might be useful if I need to find
  candidates for solving certain polynomial equations. This can also be used to
  prove that sqrt(2) is not a rational number, and several other numbers, mostly
  by doing some brute-force cases for checking all possible solutions. It's a
  bit boring to do that! Note: this theorem only applies to finding *rational*
  zeros of polynomials with *integer* coefficients. For a more general rule, use
  "Newton's method" or the "secant method."

- The set of real numbers. Now we're getting into real stuff here! We also have
  the triangle inequality, blah blah blah ...

- The Completeness Axiom. This is the assertion that "\mathbb{R} has no gaps"
  and is the key factor which distinguishes \mathbb{R} from \mathbb{Q}. (It's
  discussed in Section 4.4.) Among other things, this section discusses:

    - The concepts of a minimum, maximum, and slightly more non-trivially, those
      of an _infinum_ (greatest lower bound) and _supremum_ (least upper bound).
      For the latter two, I know clearly that sup S and inf S do not have to
      belong to S! Classic example: (a,b). I remember doing examples like these
      from MATH 305 at Williams College: basically, finding the infimums and
      supremums of sets. It's nothing too fancy. Man, I must have been a bad
      student back then!

    - The concepts of upper bounds, lower bounds, etc.

    - The completeness axiom (as I mentioned). This does _not_ hold for the
      rationals!

  Yeah, nothing too advanced here. I'm happy that at least this material is easy
  for me to understand and review.

- The symbols +infinity and -infinity, which are useful but must be handled with
  care. Do not treat them as real numbers that can be plugged into theorems!
  Note that it is also discussed that for nonempty, _bounded_ subsets A and B of
  \mathbb{R}, sup(A + B) = sup A + sup B and the same relation for infimums.
  This might be useful in some statistics proofs if we are dealing with multiple
  sets.
   
    - Useful to define sup S = +infinity if S is not bounded above, etc.

- The last section is a "Development of \mathbb{R}" and it's probably not that
  useful for me.


*************
* CHAPTER 2 *
*************

This is about sequences and is hugely critical to understanding the rest of the
book, and for real analysis in general.

Section 2.7

- Sequences are just a function from an index to some value.

- We formally define _limits_, _convergence_, and _divergence_. See the
  textbook. I won't belabor the point here. Side note: limits are unique (prove
  this by assuming two limits, then showing that |s-t| is less than epsilon
  using the definitions and then the triangle inequality).  Side note 2:
  oscillations (as in, (-1)^n) do not converge!

Section 2.8

- A discussion on proofs! When proving limits, we should invoke the formal
  definition and find n and epsilon s.t. the definition of a limit holds.

- There are several interesting examples. I did a few of them quickly. I don't
  think I will ever have to invoke these directly any time soon (I'm mostly
  reading this section so that the more important parts later are clearer to
  me).

- Exercise 8.5 is interesting, the "squeeze lemma" and I remember Professor
  Mihai Stoiciu talking about this during office hours (heh, we never had office
  hours _in_ his office since there were so many people!).

Section 2.9

- Limit theorems for sequences. I can invoke these pretty easily. I will again
  be skimming the proofs.

- Oof, there's a lot of them. Mostly they involve similar techniques such as
  working backwards and solving for the tightest bounds, so we get the lowest
  value N such that the statement: "when n > N we get |s_n - s| < epsilon" is
  true. We have to sometimes develop upper bounds, and often have to use epsilon
  times some constant so that the later algebra gets it equal to epsilon. I've
  seen this stuff many times.

Section 2.10

Monotone Sequences and Cauchy Sequences. These help us conclude convergence of
sequences _without_ knowing limits in advance.

- Monotone sequences are those which are always increasing or always decreasing.
  They _can_ converge, if the rate of increase (respectively, decrease) slows to
  zero, think of 1/x for x>0 as x grows large.

- Important Theorem I (10.2 in the book): All bounded monotone sequences
  converge.

  - Proof: let u be the supremum of the bounded sequence, so then we just show
    lim s_n = u. We start by fixing an epsilon (as usual) then we have to find
    some N such that for all n > N, we get |s_n - u| < epsilon. Well, (s_n) is
    increasing so we just need to first find an N so that u-epsilon < N <
    epsilon and then that automatically proves the statement. Yay! The proof is
    short and elegant. Again, it just relies on proving the limit statement!!

  - There's a related theorem which shows that if the sequences are unbounded,
    then, well they converge to infinity or minus infinity. (This is assuming
    monotone, because otherwise you can have oscillations to infinity, which
    would mean something different I guess.) Thus, limits of monotone sequences
    always have meaning.

- Important Theorem II (10.11 in the book): a sequence is a convergent sequence
  IFF it is a Cauchy sequence.

  - Proof: well, they did one direction earlier and it makes sense. The other
    direction also makes sense. In both cases we simply start with the
    definition and try to prove the property. They can be tricky to come up.
    Mostly it's about making sense of sup-s and thinking of "stuff plus
    epsilon."

  - Uses Definition 10.8 which defines a _Cauchy_sequence_, a sequence has this
    property if for each epsilon > 0 there exists N such that (m,n) both greater
    than N implies |s_n - s_m| < epsilon.

  - Why is it useful? Because we can confirm that a sequence converges by
    verifying that it satisfies the Cauchy sequence property. We do not have to
    explicitly compute a limit in this case!

- There's an interlude about discussions of decimals, but it's not likely to be
  much of concern to me. Don't forget about the geometric series convergence
  formula! It's 1/(1-r) for r>1. 

- There is also discussion on lim sup and lim inf. A sequence has a limit if and
  only if their `lim inf` and `lim sup` are equal. Also, lim sup is NOT
  generally sup{s_n for all n} because as N grows large, the set of elements we
  consider for lim inf gets smaller, hence the correct relationship is <=. Also,
  it's these lim inf and lim sup concepts which motivate the Cauchy sequence
  definition (see my notes above).

Section 2.11

Subsequences!!

TODO

Section 2.12

TODO


*************
* CHAPTER 3 *
*************

TODO


*************
* CHAPTER 4 *
*************

TODO


*************
* CHAPTER 5 *
*************

TODO


*************
* CHAPTER 6 *
*************

TODO


*************
* CHAPTER 7 *
*************

TODO
